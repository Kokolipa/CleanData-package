{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Dependencies\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyod.models.cd import CD\n",
    "from pyod.utils.data import get_outliers_inliers\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from spellchecker import SpellChecker\n",
    "\n",
    "from clean_package import CleanData\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m#FIXME: Insert time measurement feature to evaluate the performance of the function as a \"printed\" output to evaluate performance live (Identify Pivot opportunities)\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mCleanData\u001b[39;00m:\n\u001b[1;32m      4\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, data: pd\u001b[39m.\u001b[39mDataFrame, na: np\u001b[39m.\u001b[39mnan):\n\u001b[1;32m      5\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata \u001b[39m=\u001b[39m data\n",
      "Cell \u001b[0;32mIn[1], line 4\u001b[0m, in \u001b[0;36mCleanData\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mCleanData\u001b[39;00m:\n\u001b[0;32m----> 4\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, data: pd\u001b[39m.\u001b[39mDataFrame, na: np\u001b[39m.\u001b[39mnan):\n\u001b[1;32m      5\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata \u001b[39m=\u001b[39m data\n\u001b[1;32m      6\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mna \u001b[39m=\u001b[39m na\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "#FIXME: Insert time measurement feature to evaluate the performance of the function as a \"printed\" output to evaluate performance live (Identify Pivot opportunities)\n",
    "\n",
    "class CleanData:\n",
    "    def __init__(self, data: pd.DataFrame, na: np.nan):\n",
    "        self.data = data\n",
    "        self.na = na\n",
    "    \n",
    "    #!############################# # Memory Optimisation # ##############################\n",
    "\n",
    "    class Memory:\n",
    "        def __init__(self, data):\n",
    "            self.data = data\n",
    "        \n",
    "        \n",
    "        #* (1) Method \n",
    "        @classmethod\n",
    "        def optimise_mem(cls, data: pd.DataFrame, verbose=True) -> pd.DataFrame:\n",
    "            # Create a function to optimise the memory\n",
    "            start_mem = data.memory_usage().sum() / 1024 ** 2\n",
    "            numerics = [\"int8\", \"int16\", \"int32\", \"int64\", \"float16\", \"float32\", \"float64\"]\n",
    "            for col in data.columns:\n",
    "                col_type = data[col].dtypes\n",
    "                if col_type in numerics:\n",
    "                    # Retrieve the min and max values of a column\n",
    "                    c_min = data[col].min()\n",
    "                    c_max = data[col].max()\n",
    "                    # ? Treating integer columns\n",
    "                    if str(col_type)[:3] == \"int\":\n",
    "                        if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                            data[col] = data[col].astype(np.int8)\n",
    "                        elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                            data[col] = data[col].astype(np.int16)\n",
    "                        elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                            data[col] = data[col].astype(np.int32)\n",
    "                        elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                            data[col] = data[col].astype(np.int64)\n",
    "                    # ? Treating float columns \n",
    "                    else:\n",
    "                        if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                            data[col] = data[col].astype(np.float32)\n",
    "                        elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                            data[col] = data[col].astype(np.float32)\n",
    "                        else:\n",
    "                            data[col] = data[col].astype(np.float64)\n",
    "            # Returning the end megabytes calculation (reduction)\n",
    "            end_mem = data.memory_usage().sum() / 1024 ** 2\n",
    "\n",
    "            if verbose:\n",
    "                print(\"Mem. usage decreased to {:.2f} Mb ({:.1}% reduction)\".format(end_mem, 100 * ((start_mem - end_mem) / start_mem)))\n",
    "            \n",
    "            return data \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #!############################# # Treating NA values subclass # ##############################\n",
    "    \n",
    "    class TreatNA:\n",
    "        def __init__(self, data):\n",
    "            self.data = data\n",
    "\n",
    "        \n",
    "        #* (1) Method \n",
    "        @classmethod\n",
    "        def IdentifyNAs(cls, data: pd.DataFrame) -> pd.DataFrame:\n",
    "            missing_values = [np.nan, 'missing', 'null', '', 'empty']\n",
    "            # Find rows containing any of the missing values\n",
    "            mask = data.apply(lambda row: any(str(val) in missing_values for val in row), axis=1)\n",
    "            return data[mask]\n",
    "\n",
    "        \n",
    "        #* (2) Method\n",
    "        @classmethod\n",
    "        def complete_case_na(cls, data: pd.DataFrame) -> pd.DataFrame:\n",
    "            # Include edge cases\n",
    "            missing_values = ['missing', 'null', '', 'empty']\n",
    "\n",
    "            # Filter the data to return CCA with edge cases\n",
    "            return data[data.apply(lambda row: any(pd.isna(val) or str(val) in missing_values for val in row), axis=1)]\n",
    "\n",
    "        \n",
    "        #* (3) Method \n",
    "        @classmethod\n",
    "        def drop_complete_case_na(cls, data: pd.DataFrame) -> pd.DataFrame:\n",
    "            # Identify rows to drop and return the corresponding index value\n",
    "            missing_values = ['missing', 'null', '', 'empty']\n",
    "            rows_to_drop = data[data.apply(lambda row: any(pd.isna(val) or str(val) in missing_values for val in row), axis=1)].index\n",
    "\n",
    "            # Drop rows\n",
    "            cleaned_data = data.drop(index=rows_to_drop)\n",
    "            return cleaned_data\n",
    "        \n",
    "        \n",
    "        #* (4) Method \n",
    "        @classmethod\n",
    "        def DataImpute(cls, data: pd.DataFrame, features: list, missing_values=np.nan, numeric_strategy='mean', string_strategy='constant', string_fill_value=None):\n",
    "            \"\"\"Apply univariate data imputation for numerical & categorical strategies. Suitable for MCAR cases (A variable is missing completely at random (MCAR) if the probability of being missing is the same for all the observations)\n",
    "\n",
    "            Args:\n",
    "                - data (pd.DataFrame)\n",
    "                - features (list)\n",
    "                - missing_values (the placeholder for the missing values): **Defaults to np.nan;  Can also be pd.NA, int, float,  or str. \n",
    "                - numeric_strategy (str, optional): _description_. Defaults to 'mean'; Numerical data imputation (strategy).\n",
    "                - string_strategy (str, optional): _description_. Defaults to 'constant'. Categorical data imputation (strategy).\n",
    "                - string_fill_value (_type_, optional): When strategy == “constant”, fill_value is used to replace all occurrences of missing_values. Defaults to None.\n",
    "\n",
    "            Returns:\n",
    "                _type_: pd.DataFrame\n",
    "            \"\"\"            \n",
    "            imputed_data = data.copy()\n",
    "            numeric_features = list(data.select_dtypes(include=np.number).columns)\n",
    "            string_features = list(data.select_dtypes(include=object).columns)\n",
    "\n",
    "            # Numeric imputation: Mean, Median, and Mode\n",
    "            if any(feature in numeric_features for feature in features):\n",
    "                numeric_imputer = SimpleImputer(strategy=numeric_strategy, missing_values=missing_values)\n",
    "                imputed_data[numeric_features] = numeric_imputer.fit_transform(data[numeric_features])\n",
    "\n",
    "            # String/object imputation: Custom (user based)\n",
    "            if any(feature in string_features for feature in features):\n",
    "                string_imputer = SimpleImputer(strategy=string_strategy, fill_value=string_fill_value, missing_values=missing_values)\n",
    "                imputed_data[string_features] = string_imputer.fit_transform(data[string_features])\n",
    "\n",
    "            return imputed_data\n",
    "        \n",
    "        \n",
    "        #* (5) Method \n",
    "        @classmethod\n",
    "        def MNAR(cls, data:pd.DataFrame , features:list) -> pd.DataFrame:\n",
    "            \"\"\"Missing of values is not at random (MNAR) if their being missing depends on information not recorded in the dataset. Transaction dataset = the values are missing if if we don't have transaction_number (NOTE: here we could have more the one independent variable)\n",
    "            NOTE: **Independent = transaction_number; **dependent (in their occurance): the rest of the variables\n",
    "\n",
    "            Args:\n",
    "                data (_type_): pd.DataFrame / np.array (2d array)\n",
    "\n",
    "            Returns:\n",
    "                pd.DataFrame: This function will drop all corresponsing NA values from the dependent variables based on the Independent variable/s\n",
    "            \"\"\"            \n",
    "            return data.loc[:, features].dropna()\n",
    "        \n",
    "        # NOTE: if the occurrance of missing values is dependent on a certain value within a class for example the in our dataset, girls will not disclose their weight in some sort of ages\n",
    "        # - Spark beyond - Israel \n",
    "        # - Use logisitic regression to evaluate soft predictions for each of the variables.\n",
    "        \"\"\"Maybe what I can do is plot to features based on the project, dataset and purpose of the analysis.\n",
    "            Based on that explore the feature that has missing values against other features and identify interaction of the boolean representation (is_na as 'hue')\n",
    "            for the feature the indicated interaction, based on a user input using the **input** function + extracting the user input value as the feature independent\n",
    "            variable and then fill in the gaps based on the requested with another **input** function\"\"\"\n",
    "        @classmethod\n",
    "        def logistic_regression_MAR_identifier(cls,df: pd.DataFrame, max_iter=1000) -> pd.DataFrame:\n",
    "            # Identify columns containing NA/NaN values\n",
    "            columns_with_na = df.columns[df.isna().any()].tolist()\n",
    "\n",
    "            # Create NA flags columns\n",
    "            flag_columns = {f'Flag {i+1}': df[col].isna().astype(int) for i, col in enumerate(columns_with_na)}\n",
    "\n",
    "            # Vertically append the flags to the data frame\n",
    "            df = pd.concat([df, pd.DataFrame(flag_columns)], axis=1)\n",
    "\n",
    "            # Drop the columns that contain NA values\n",
    "            df.drop(columns=columns_with_na, inplace=True)\n",
    "\n",
    "            # Run logistic regression for evaluation\n",
    "            results = []  # Initialize an empty list to store results\n",
    "            for col in df.columns:\n",
    "                X = pd.get_dummies(df.drop(columns=[col]), drop_first=True)\n",
    "                y = df[col]\n",
    "\n",
    "                # Check if y is binary/multi-class categorical for logistic regression\n",
    "                if y.nunique() <= 2:\n",
    "                    X_scaled = StandardScaler().fit_transform(X)\n",
    "                    try:\n",
    "                        lr = LogisticRegression(max_iter=max_iter, n_jobs=-1).fit(X_scaled, y)\n",
    "                        # Collect coefficients along with the column name\n",
    "                        for idx, coef in enumerate(lr.coef_[0]):\n",
    "                            results.append({'Column': col, 'Feature': X.columns[idx], 'Coefficient': float(coef)})\n",
    "                    except ValueError as e:\n",
    "                        print(f\"Error fitting model with target {col}: {e}\")\n",
    "                else:\n",
    "                    print(f\"Skipping '{col.upper()}' - not suitable for logistic regression.\")\n",
    "\n",
    "            # Convert results to DataFrame for easier analysis and return it\n",
    "            results_df = pd.DataFrame(results)\n",
    "\n",
    "            # Specify data types for each column\n",
    "            results_df = results_df.astype({'Column': 'str', 'Feature': 'str', 'Coefficient': 'float'})\n",
    "\n",
    "            return results_df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #!############################# # Treat Duplicated Values Class # ##############################\n",
    "\n",
    "    class FindTreatDuplicates:\n",
    "        def __init__(self, data):\n",
    "            self.data = data\n",
    "        \n",
    "        \n",
    "        #* (1) Method \n",
    "        @classmethod\n",
    "        def find_duplicates(cls, data: pd.DataFrame, subset=None, identify_all=False) -> pd.DataFrame:\n",
    "            \"\"\"Idenfify duplicated values in DataFrame. \n",
    "            Args:\n",
    "                data (pd.DataFrame): pd.DataFrame\n",
    "                subset (list | pd.Series): A list of features OR a singular searies. Default = None (return duplicates for the intire dataset).\n",
    "                identify_all (bool, optional): If 'first' specified, then return only the first instances of the duplicated values. Defaults to False (identify all duplicated values). If 'last' return only the last instances of the duplicated values. Defaults to False (identify all duplicated values)\n",
    "            \"\"\"\n",
    "            return data[data.duplicated(subset=subset, keep=identify_all)]            \n",
    "        \n",
    "        \n",
    "        #* (2) Method \n",
    "        @classmethod\n",
    "        def drop_duplicates(cls, data: pd.DataFrame, subset=None, identify_all='first') -> pd.DataFrame:\n",
    "            \"\"\"Drop duplicated values in DataFrame. \n",
    "            Args:\n",
    "                data (pd.DataFrame): pd.DataFrame\n",
    "                subset (list | pd.Series): A list of features OR a singular searies. Default = None (applies to all features).\n",
    "                identify_all (bool, optional): Defaults to 'first' (drop all duplicated values but keep the 'first' instances); If 'last' return only the last instances of the duplicated values; If False = drop all duplicates.\n",
    "            \"\"\"\n",
    "            return data.drop_duplicates(subset=subset, keep=identify_all, inplace=True)       \n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    #!############################# # Find & Treat Text Typos # ##############################\n",
    "\n",
    "    class TextTypos:\n",
    "        def __init__(self, data: pd.DataFrame):\n",
    "            self.data = data\n",
    "        \n",
    "        #* (1) Method\n",
    "        @classmethod\n",
    "        def strip_and_lower_strings(cls, data: pd.DataFrame) -> pd.DataFrame: \n",
    "            for col in data.select_dtypes(exclude=\"number\").columns:\n",
    "                data[col] = data[col].str.lower()\n",
    "                if data[col].str.startswith(\" \").any() or data[col].str.endswith(\" \").any():\n",
    "                    data[col] = data[col].str.strip()\n",
    "            return data\n",
    "\n",
    "        \n",
    "        #* (2) Method \n",
    "        @classmethod\n",
    "        def object_to_numeric(cls,df:pd.DataFrame, features: list) -> pd.DataFrame:\n",
    "            for col in df[features].columns:\n",
    "                df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "            return df[col]\n",
    "        \n",
    "        \n",
    "        #* (3) Method \n",
    "        @classmethod\n",
    "        def correct_word(cls, word: str) -> str:\n",
    "            spell = SpellChecker()\n",
    "            return spell.correction(word)\n",
    "        \n",
    "        \n",
    "        #* (4) Method \n",
    "        @classmethod\n",
    "        def correct_sentence(cls, strings:str) -> str:\n",
    "            words = strings.split(' ')  # Split the string into words\n",
    "            corrected_words = [cls.correct_word(word) for word in words]  # Apply correction to each word\n",
    "            return ' '.join(corrected_words)  \n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    #!############################# # Find & Treat Anomalies # ##############################\n",
    "    #FIXME: finalise Anomalies class -> Create a function to find categorical Anomalies with value_counts(normalise=True). \n",
    "\n",
    "    class Anomalies:\n",
    "        def __init__(self, data):\n",
    "            self.data = data\n",
    "\n",
    "        \n",
    "        #* (1) Method\n",
    "        @classmethod # class method for date anomalies\n",
    "        def find_date_anomalies(cls, data:pd.DataFrame, date_column: str, identify_by='month'):\n",
    "            \"\"\"_summary_\n",
    "\n",
    "            Args:\n",
    "                data (pd.DataFrame): _description_\n",
    "                date_column (str): _description_\n",
    "                identify_by (str, optional): _description_. Defaults to 'month'.\n",
    "\n",
    "            Returns:\n",
    "                _type_: _description_\n",
    "            \"\"\"            \n",
    "\n",
    "            if identify_by == 'month':\n",
    "                data['year'] = data[date_column].dt.year\n",
    "                data['month'] = data[date_column].dt.month\n",
    "                eval_data = data.groupby(['year', 'month']).size().reset_index(name=\"days_count\")\n",
    "                return eval_data.loc[eval_data['days_count'] < 28]\n",
    "            \n",
    "            elif identify_by == 'year':\n",
    "                # Create year & month features\n",
    "                data['month'] = data[date_column].dt.month\n",
    "                data['year'] = data[date_column].dt.year\n",
    "                eval_data = data.groupby(['year', 'month']).size().reset_index(name=\"days_count\")\n",
    "                year_count = eval_data.groupby('year')['days_count'].sum().reset_index()\n",
    "                year_missing = year_count[year_count['days_count'] < 365]\n",
    "\n",
    "                # Print statements for missing days in each year\n",
    "                for index, row in year_missing.iterrows():\n",
    "                    missing_days = 365 - row['days_count']\n",
    "                    print(f\"Year {row['year']} is missing {missing_days} days ({round(missing_days/30, 2)} month/s)\")\n",
    "        \n",
    "        \n",
    "        #* (2) Method\n",
    "        @classmethod\n",
    "        def nonlinear_outliers_influencers_knn(cls, data: pd.DataFrame, features: list, neighbors_fraction: float = 0.1, contamination='auto', center_measure='mean'):\n",
    "            \"\"\"Detects outliers in a dataset based on nonlinear methods and KNN.\n",
    "\n",
    "            Args:\n",
    "                data (pd.DataFrame): The dataset to analyze.\n",
    "                features (list): List of features to consider for outlier detection.\n",
    "                neighbors_fraction (float, optional): Fraction of dataset size to use as neighbors. Defaults to 0.1.\n",
    "                contamination (str, optional): Method for calculating contamination ('auto', '3std'). Defaults to 'auto'.\n",
    "                center_measure (str, optional): Central distribution measure to use ('mean' or 'median'). Defaults to 'mean'.\n",
    "\n",
    "            Returns:\n",
    "                pd.DataFrame: DataFrame of outliers.\n",
    "            \"\"\"    \n",
    "            \n",
    "            if contamination == 'auto': \n",
    "                # Start the timer\n",
    "                start_time = time.time()\n",
    "                \n",
    "                # Calculate the number of neighbors based on a fraction of the dataset size\n",
    "                n_neighbors = max(1, int(len(data) * neighbors_fraction))\n",
    "\n",
    "                # Use Local Outlier Factor for outlier detection (contamination auto = 0.1)\n",
    "                clf = LocalOutlierFactor(n_neighbors=n_neighbors, contamination='auto')\n",
    "                X = data[features]\n",
    "                # Fit the model and predict outliers (-1 for outliers, 1 for inliers)\n",
    "                y_pred = clf.fit_predict(X)\n",
    "                \n",
    "                # Filter outliers\n",
    "                X['outlier'] = y_pred\n",
    "                outliers = X[X['outlier'] == -1]\n",
    "                \n",
    "                # End the timer\n",
    "                end_time = time.time()\n",
    "                # Calculate the elapsed time\n",
    "                elapsed_time = end_time - start_time\n",
    "                \n",
    "                # Convert elapsed time to milliseconds\n",
    "                elapsed_time_ms = elapsed_time * 1000\n",
    "\n",
    "                # Print the elapsed time in milliseconds\n",
    "                print(\"Elapsed time:\", elapsed_time_ms, \"milliseconds\")\n",
    "                print(\"n_nighbors:\", n_neighbors)\n",
    "                return outliers.drop('outlier', axis=1)\n",
    "            \n",
    "            elif contamination == '3std':\n",
    "                # Start the timer\n",
    "                start_time = time.time()\n",
    "                \n",
    "                # Calculate the mean and standard deviation of features\n",
    "                if center_measure == 'mean':\n",
    "                    centers = data[features].mean().values\n",
    "                    spreads = data[features].std().values\n",
    "                elif center_measure == 'median':\n",
    "                    centers = data[features].median().values\n",
    "                    spreads = data[features].std().values  \n",
    "                        \n",
    "                # Calculate contamination -> return 3 STD from the mean +/-\n",
    "                contamination_values = []\n",
    "                for i, spread in enumerate(spreads):\n",
    "                    if center_measure == 'mean':\n",
    "                        outlier_mask = (data.iloc[:, i] < centers[i] - 3 * spread) | (data.iloc[:, i] > centers[i] + 3 * spread)\n",
    "                    elif center_measure == 'median':\n",
    "                        outlier_mask = (data.iloc[:, i] < centers[i] - 3 * spread) | (data.iloc[:, i] > centers[i] + 3 * spread)\n",
    "                    contamination_values.append(data[outlier_mask].shape[0] / data.shape[0])\n",
    "                        \n",
    "                # Return contamination\n",
    "                contamination = np.median(contamination_values)\n",
    "                \n",
    "                # Calculate the number of neighbors based on a fraction of the dataset size\n",
    "                n_neighbors = max(1, int(len(data) * neighbors_fraction))\n",
    "\n",
    "                # Use Local Outlier Factor for outlier detection (contamination auto = 3 STD away from the mean +/-) \n",
    "                clf = LocalOutlierFactor(n_neighbors=n_neighbors, contamination=contamination)\n",
    "                X = data[features]\n",
    "                # Fit the model and predict outliers (-1 for outliers, 1 for inliers)\n",
    "                y_pred = clf.fit_predict(X)\n",
    "                \n",
    "                # Filter outliers\n",
    "                X['outlier'] = y_pred\n",
    "                outliers = X[X['outlier'] == -1]\n",
    "                \n",
    "                # End the timer\n",
    "                end_time = time.time()\n",
    "                # Calculate the elapsed time\n",
    "                elapsed_time = end_time - start_time\n",
    "                \n",
    "                # Convert elapsed time to milliseconds\n",
    "                elapsed_time_ms = elapsed_time * 1000\n",
    "\n",
    "                # Print the elapsed time in milliseconds\n",
    "                print(\"Elapsed time:\", elapsed_time_ms, \"milliseconds\")\n",
    "                print(\"n_nighbors:\", n_neighbors)\n",
    "                return outliers.drop('outlier', axis=1)\n",
    "            else:\n",
    "                raise ValueError(\"Invalid value for contamination. Please provide 'auto' or '3std'.\")\n",
    "\n",
    "        \n",
    "        #* (3) Method\n",
    "        @classmethod\n",
    "        def linear_outliers_influencers(cls,data:pd.DataFrame ,features:list, center_measure='mean'):\n",
    "            \"\"\"This function align for linear datasets to explore outliers using Cook's D (distance based evaluation). A Cook’s result > 1 = Significant influence, while Cook’s D > 0.5 is worth investigating. \n",
    "\n",
    "            Args:\n",
    "                data (pd.DataFrame): The dataset to analyze.\n",
    "                features (list): List of features to consider for outlier detection.\n",
    "                center_measure (str, optional): Central distribution measure to use ('mean' or 'median'). Defaults to 'mean'.\n",
    "\n",
    "            Returns:\n",
    "                pd.DataFrame: DataFrame of outliers.\n",
    "            \"\"\"            \n",
    "            \n",
    "            # Calculate the mean and standard deviation of features\n",
    "            if center_measure == 'mean':\n",
    "                centers = data[features].mean().values\n",
    "                spreads = data[features].std().values\n",
    "            elif center_measure == 'median':\n",
    "                centers = data[features].median().values\n",
    "                spreads = data[features].std().values  \n",
    "                        \n",
    "            # Calculate contamination -> return 3 STD from the mean +/-\n",
    "            contamination_values = []\n",
    "            for i, spread in enumerate(spreads):\n",
    "                if center_measure == 'mean':\n",
    "                    outlier_mask = (data.iloc[:, i] < centers[i] - 3 * spread) | (data.iloc[:, i] > centers[i] + 3 * spread)\n",
    "                elif center_measure == 'median':\n",
    "                    outlier_mask = (data.iloc[:, i] < centers[i] - 3 * spread) | (data.iloc[:, i] > centers[i] + 3 * spread)\n",
    "                contamination_values.append(data[outlier_mask].shape[0] / data.shape[0])\n",
    "                        \n",
    "            # Return contamination\n",
    "            contamination = np.median(contamination_values)\n",
    "\n",
    "            # Splitting to X & Y columns\n",
    "            X = data[features]\n",
    "\n",
    "            # Instantiate the Cook's D evaluator\n",
    "            cooks_D = CD(contamination=contamination)\n",
    "            # predict outliers\n",
    "            cooks_D.fit(X)\n",
    "\n",
    "            # predict outliers & inliners (bool array = 1/0)\n",
    "            pred = cooks_D.predict(X, return_confidence=True)\n",
    "            # We have to transpose the dataset to match the dataset\n",
    "            df = pd.DataFrame(pred).T\n",
    "            df = df.rename(columns={0: \"predictions\", 1: \"confidence\"})\n",
    "\n",
    "            X_array = X.to_numpy() # Transforming to numpy array to leverage get_outliers_inliers\n",
    "            y_array = df[\"predictions\"]   \n",
    "                \n",
    "            # Extracting inliners and outliers\n",
    "            X_outliers, X_inliners = get_outliers_inliers(X_array, y_array)\n",
    "\n",
    "            # Result outliers\n",
    "            return pd.DataFrame(X_outliers, columns=X.columns)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
